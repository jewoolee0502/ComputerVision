{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtZ35y73ujSsR5wiSgD3zO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jewoolee0502/ComputerVision/blob/main/ECSE415_A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ECSE 415: Introduction to Computer Vision\n",
        "###### Jewoo Lee - 260910789\n",
        "###### Anthony Bonta - 261053688"
      ],
      "metadata": {
        "id": "iAzPZSvMwqs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 5: Video Analysis"
      ],
      "metadata": {
        "id": "1CuxHBzrwvP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library Requirements"
      ],
      "metadata": {
        "id": "O_3pqgqmw7vk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz6zb0gpwXbw",
        "outputId": "d3fc6dd7-4f82-428f-8af6-f4abff5e7958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Installs\n",
        "!pip install -q ultralytics\n",
        "!pip install -q ultralytics deep-sort-realtime\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from scipy.optimize import linear_sum_assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Path"
      ],
      "metadata": {
        "id": "Q1d94JUdw-tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/'\n",
        "\n",
        "object_tracking_root = os.path.join(path, \"Object_Tracking\")"
      ],
      "metadata": {
        "id": "Xwnohvxfw-9B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Preparation"
      ],
      "metadata": {
        "id": "8V6aP8E-xBps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task1_images_dir = os.path.join(object_tracking_root, \"Task1\", \"images\")\n",
        "task1_video_path = os.path.join(object_tracking_root, \"task1_input.mp4\")\n",
        "\n",
        "print(\"Task1 images dir:\", task1_images_dir)\n",
        "print(\"Output video path:\", task1_video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdEnjVrYxF3p",
        "outputId": "cbd6d9e9-ba6b-458e-8626-2c1efce8174f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task1 images dir: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/Task1/images\n",
            "Output video path: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task1_input.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FPS = 14 # given fps value\n",
        "\n",
        "def images_to_video(images_dir, output_path, fps):\n",
        "  # all images are .jpg\n",
        "  image_files = sorted(glob.glob(os.path.join(images_dir, \"*.jpg\")))\n",
        "  print(f\"Found {len(image_files)} images in {images_dir}\")\n",
        "\n",
        "  if len(image_files) == 0:\n",
        "    raise RuntimeError(f\"No .jpg image files found in {images_dir}\")\n",
        "\n",
        "  # read the first image and get its dimensions\n",
        "  first_img = cv2.imread(image_files[0])\n",
        "  height, width = first_img.shape[:2]\n",
        "  frame_size = (width, height)\n",
        "  print(f\"Target frame size: {frame_size}\")\n",
        "\n",
        "  # set up the video writer\n",
        "  fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "  writer = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
        "\n",
        "  if not writer.isOpened():\n",
        "    raise RuntimeError(f\"VideoWriter could not be opened for {output_path}\")\n",
        "\n",
        "  # write all images as frames\n",
        "  for idx, img_path in enumerate(image_files):\n",
        "    frame = cv2.imread(img_path)\n",
        "\n",
        "    if frame is None:\n",
        "      print(f\"Skipping unreadable image: {img_path}\")\n",
        "      continue\n",
        "\n",
        "    # resize every frame matches the first image's size\n",
        "    frame = cv2.resize(frame, frame_size)\n",
        "    writer.write(frame)\n",
        "\n",
        "  writer.release()\n",
        "  print(f\"Video saved to: {output_path}\")\n",
        "\n",
        "images_to_video(task1_images_dir, task1_video_path, FPS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBk9hhpbDPbU",
        "outputId": "954e2ca4-d21c-4786-eb44-371e38fb13f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 429 images in /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/Task1/images\n",
            "Target frame size: (1920, 1080)\n",
            "Video saved to: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task1_input.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Model Implementation"
      ],
      "metadata": {
        "id": "ujFwagwzxGYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_video_path = os.path.join(object_tracking_root, \"task1.mp4\")\n",
        "output_txt_path = os.path.join(object_tracking_root, \"task1.txt\")\n",
        "\n",
        "print(\"Output video path:\", output_video_path)\n",
        "print(\"Output text path:\", output_txt_path)\n",
        "\n",
        "def draw_and_log_box(frame, frame_idx, track, txt_file, color=(0, 0, 255)):\n",
        "  track_id = int(track.track_id)\n",
        "  x1, y1, x2, y2 = track.to_ltrb()\n",
        "\n",
        "  x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "  w = x2 - x1\n",
        "  h = y2 - y1\n",
        "\n",
        "  # draw bounding box\n",
        "  cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "  # draw label slightly above the box\n",
        "  label_pos = (x1, max(0, y1 - 8))\n",
        "  cv2.putText(\n",
        "      frame,\n",
        "      f\"ID: {track_id}\",\n",
        "      label_pos,\n",
        "      cv2.FONT_HERSHEY_SIMPLEX,\n",
        "      0.5,\n",
        "      color,\n",
        "      2\n",
        "  )\n",
        "\n",
        "  # write tracking line\n",
        "  if txt_file is not None:\n",
        "    txt_file.write(f\"{frame_idx},{track_id},{x1},{y1},{w},{h}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ldGwBJXxI6b",
        "outputId": "dda5dd10-4360-4d0b-85f8-2acc4b1bf951"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output video path: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task1.mp4\n",
            "Output text path: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_threshold = 0.3\n",
        "\n",
        "yolo_model = YOLO(\"yolov8s.pt\")\n",
        "tracker = DeepSort(max_age=30, n_init=3, max_iou_distance=0.8)\n",
        "\n",
        "cap = cv2.VideoCapture(task1_video_path) # open the input video\n",
        "if not cap.isOpened():\n",
        "  raise RuntimeError(\"Cannot open video\")\n",
        "\n",
        "# output video dimensions\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fps_val = cap.get(cv2.CAP_PROP_FPS)\n",
        "if fps_val <= 0:\n",
        "  fps_val = FPS\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "writer = cv2.VideoWriter(output_video_path, fourcc, fps_val, (w, h))\n",
        "\n",
        "txt_file = open(output_txt_path, \"w\")\n",
        "\n",
        "tracks_memory = []\n",
        "frame_idx = 1\n",
        "\n",
        "while True:\n",
        "  ok, frame = cap.read()\n",
        "  if not ok:\n",
        "    break\n",
        "\n",
        "  # run the YOLO model\n",
        "  det = yolo_model(frame, conf=conf_threshold, verbose=False)[0]\n",
        "\n",
        "  # convert YOLO detections into DeepSORT format\n",
        "  det_list = []\n",
        "  if det.boxes is not None:\n",
        "    for b in det.boxes:\n",
        "      cls_id = int(b.cls[0])\n",
        "      conf = float(b.conf[0])\n",
        "\n",
        "      if cls_id != 0:\n",
        "        continue\n",
        "\n",
        "      x1, y1, x2, y2 = b.xyxy[0].tolist()\n",
        "      det_list.append(([x1, y1, x2 - x1, y2 - y1], conf, \"person\"))\n",
        "\n",
        "  # track\n",
        "  tracks = tracker.update_tracks(det_list, frame=frame)\n",
        "\n",
        "  # draw & log only confirmed tracks\n",
        "  for trk in tracks:\n",
        "    if not trk.is_confirmed():\n",
        "      continue\n",
        "\n",
        "    draw_and_log_box(frame, frame_idx, trk, txt_file)\n",
        "\n",
        "    # saving in memory\n",
        "    x1, y1, x2, y2 = trk.to_ltrb()\n",
        "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "    tracks_memory.append((frame_idx, trk.track_id, x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "  writer.write(frame)\n",
        "\n",
        "  if frame_idx % 50 == 0:\n",
        "    print(\"Processed Frames:\", frame_idx)\n",
        "\n",
        "  frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "writer.release()\n",
        "txt_file.close()\n",
        "\n",
        "print(\"\\nCompleted!\")\n",
        "print(\"Video saved to:\", output_video_path)\n",
        "print(\"Text file saved to:\", output_txt_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE85p11G_2Fz",
        "outputId": "e1d273ad-0d8f-4c0c-d466-8a0e3575cb67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% ━━━━━━━━━━━━ 21.5MB 311.2MB/s 0.1s\n",
            "Processed Frames: 50\n",
            "Processed Frames: 100\n",
            "Processed Frames: 150\n",
            "Processed Frames: 200\n",
            "Processed Frames: 250\n",
            "Processed Frames: 300\n",
            "Processed Frames: 350\n",
            "Processed Frames: 400\n",
            "\n",
            "Completed!\n",
            "Video saved to: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task1.mp4\n",
            "Text file saved to: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Evaluation"
      ],
      "metadata": {
        "id": "GnFmDPAlxJi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bbox_iou(box_a, box_b):\n",
        "  ax, ay, aw, ah = box_a\n",
        "  bx, by, bw, bh = box_b\n",
        "\n",
        "  # convert to corner coordinates\n",
        "  ax2, ay2 = ax + aw, ay + ah\n",
        "  bx2, by2 = bx + bw, by + bh\n",
        "\n",
        "  # find intersection rectangle coordinates\n",
        "  ix1 = max(ax, bx)\n",
        "  iy1 = max(ay, by)\n",
        "  ix2 = min(ax2, bx2)\n",
        "  iy2 = min(ay2, by2)\n",
        "\n",
        "  # calculate intersection area\n",
        "  iw = max(0.0, ix2 - ix1)\n",
        "  ih = max(0.0, iy2 - iy1)\n",
        "  inter = iw * ih\n",
        "\n",
        "  if inter <= 0:\n",
        "    return 0.0\n",
        "\n",
        "  # calculate union area\n",
        "  area_a = aw * ah\n",
        "  area_b = bw * bh\n",
        "  union = area_a + area_b - inter\n",
        "\n",
        "  if union <= 0:\n",
        "    return 0.0\n",
        "\n",
        "  # iou ratio\n",
        "  return inter / union"
      ],
      "metadata": {
        "id": "rpMN0RS3xLM4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tracks_file(file_path):\n",
        "  tracks = {}\n",
        "\n",
        "  with open(file_path, \"r\") as f:\n",
        "    for line in f:\n",
        "      parts = line.strip().split(\",\")\n",
        "      if len(parts) < 6:\n",
        "        continue\n",
        "\n",
        "      # parse CSV fields\n",
        "      frame_idx = int(float(parts[0]))\n",
        "      obj_id = int(float(parts[1]))\n",
        "      x = float(parts[2])\n",
        "      y = float(parts[3])\n",
        "      w = float(parts[4])\n",
        "      h = float(parts[5])\n",
        "\n",
        "      # store\n",
        "      box = (x, y, w, h)\n",
        "      if frame_idx not in tracks:\n",
        "        tracks[frame_idx] = []\n",
        "\n",
        "      tracks[frame_idx].append((obj_id, box))\n",
        "\n",
        "  return tracks"
      ],
      "metadata": {
        "id": "gC3WCOMgPlOB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mota(gt_tracks, pred_tracks, iou_threshold=0.5):\n",
        "  # initialize\n",
        "  total_fn = 0\n",
        "  total_fp = 0\n",
        "  total_idsw = 0\n",
        "  total_gt = 0\n",
        "\n",
        "  last_track_for_gt = {}   # gt_id -> pred_id\n",
        "\n",
        "  all_frames = sorted(set(gt_tracks.keys()) | set(pred_tracks.keys()))\n",
        "\n",
        "  for frame in all_frames:\n",
        "    gt_list = gt_tracks.get(frame, [])\n",
        "    pred_list = pred_tracks.get(frame, [])\n",
        "\n",
        "    num_gt = len(gt_list)\n",
        "    num_pred = len(pred_list)\n",
        "    total_gt += num_gt\n",
        "\n",
        "    # edge cases\n",
        "    if num_gt == 0 and num_pred == 0:\n",
        "      continue\n",
        "    if num_gt == 0:\n",
        "      total_fp += num_pred\n",
        "      continue\n",
        "    if num_pred == 0:\n",
        "      total_fn += num_gt\n",
        "      continue\n",
        "\n",
        "    # iou matrix: rows = GT, cols = predictions\n",
        "    iou_mat = np.zeros((num_gt, num_pred), dtype=float)\n",
        "    for gi, (gt_id, gt_box) in enumerate(gt_list):\n",
        "      for pj, (pr_id, pr_box) in enumerate(pred_list):\n",
        "        iou_mat[gi, pj] = bbox_iou(gt_box, pr_box)\n",
        "\n",
        "    # Hungarian algorithm to find optimal GT-prediction matching\n",
        "    cost_mat = 1.0 - iou_mat\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_mat)\n",
        "\n",
        "    matched_gt_idx = set()\n",
        "    matched_pred_idx = set()\n",
        "    gt_to_pred_idx = {}\n",
        "\n",
        "    # accept only pairs with iou >= threshold\n",
        "    for r, c in zip(row_ind, col_ind):\n",
        "      if iou_mat[r, c] >= iou_threshold:\n",
        "        matched_gt_idx.add(r)\n",
        "        matched_pred_idx.add(c)\n",
        "        gt_to_pred_idx[r] = c\n",
        "\n",
        "    # false negatives: GT not matched\n",
        "    total_fn += (num_gt - len(matched_gt_idx))\n",
        "\n",
        "    # false positives: predictions not matched\n",
        "    total_fp += (num_pred - len(matched_pred_idx))\n",
        "\n",
        "    # identity switches\n",
        "    for gi, pi in gt_to_pred_idx.items():\n",
        "      gt_id, _ = gt_list[gi]\n",
        "      pr_id, _ = pred_list[pi]\n",
        "\n",
        "      prev_pr_id = last_track_for_gt.get(gt_id)\n",
        "      if prev_pr_id is not None and prev_pr_id != pr_id:\n",
        "        total_idsw += 1\n",
        "\n",
        "      last_track_for_gt[gt_id] = pr_id\n",
        "\n",
        "  # mota calculation\n",
        "  if total_gt == 0:\n",
        "    mota = 0.0\n",
        "  else:\n",
        "    mota = 1.0 - (total_fn + total_fp + total_idsw) / total_gt\n",
        "\n",
        "  return mota, total_fn, total_fp, total_idsw, total_gt"
      ],
      "metadata": {
        "id": "x3F2cZlfPnpv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paths\n",
        "gt_path = os.path.join(object_tracking_root, \"Task1\", \"gt\", \"gt.txt\")\n",
        "pred_path = output_txt_path\n",
        "\n",
        "print(\"GT file:\", gt_path)\n",
        "print(\"Pred file:\", pred_path)\n",
        "\n",
        "# load data\n",
        "gt_data = load_tracks_file(gt_path)\n",
        "pred_data = load_tracks_file(pred_path)\n",
        "\n",
        "# compute MOTA\n",
        "mota, FN, FP, IDSW, GT_total = compute_mota(gt_data, pred_data, iou_threshold=0.5)\n",
        "\n",
        "print(\"\\n=== MOTA evaluation for Task 1 ===\")\n",
        "print(\"Total GT objects:\", GT_total)\n",
        "print(\"False Negatives:\", FN)\n",
        "print(\"False Positives:\", FP)\n",
        "print(\"ID Switches:\", IDSW)\n",
        "print(f\"MOTA: {mota:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8cU9_eZPrhD",
        "outputId": "ec108eb4-4a53-4734-d340-132f14b7d5fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GT file: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/Task1/gt/gt.txt\n",
            "Pred file: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task1.txt\n",
            "\n",
            "=== MOTA evaluation for Task 1 ===\n",
            "Total GT objects: 26647\n",
            "False Negatives: 17228\n",
            "False Positives: 2011\n",
            "ID Switches: 242\n",
            "MOTA: 0.2689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Prediction & Kaggle Competition"
      ],
      "metadata": {
        "id": "yQtzOFvLxLsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task2 image folder and input video path\n",
        "task2_images_dir = os.path.join(object_tracking_root, \"Task2\", \"images\")\n",
        "task2_input_video_path = os.path.join(object_tracking_root, \"task2_input.mp4\")\n",
        "\n",
        "print(\"Task2 images dir :\", task2_images_dir)\n",
        "print(\"Task2 input video:\", task2_input_video_path)\n",
        "\n",
        "# use the same helper from Part 1\n",
        "images_to_video(task2_images_dir, task2_input_video_path, FPS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi8R5N4rxPlo",
        "outputId": "85703b8a-babf-48cd-b169-8eceff13e0b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task2 images dir : /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/Task2/images\n",
            "Task2 input video: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task2_input.mp4\n",
            "Found 1050 images in /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/Task2/images\n",
            "Target frame size: (1920, 1080)\n",
            "Video saved to: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task2_input.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output paths for Task2\n",
        "task2_output_video = os.path.join(object_tracking_root, \"task2.mp4\")\n",
        "task2_counts_csv   = os.path.join(object_tracking_root, \"group50_object_tracking.csv\")\n",
        "\n",
        "print(\"Task2 annotated video:\", task2_output_video)\n",
        "print(\"Task2 counts csv:\", task2_counts_csv)\n",
        "\n",
        "conf_threshold = 0.1\n",
        "\n",
        "# if you want, you can reuse existing yolo_model / tracker,\n",
        "# or re-create them here (same as Part 2):\n",
        "yolo_model = YOLO(\"yolov8l.pt\")\n",
        "tracker = DeepSort(max_age=30, n_init=3, max_iou_distance=0.7)\n",
        "\n",
        "cap2 = cv2.VideoCapture(task2_input_video_path)\n",
        "if not cap2.isOpened():\n",
        "    raise RuntimeError(\"Cannot open Task2 input video\")\n",
        "\n",
        "w2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h2 = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fps2 = cap2.get(cv2.CAP_PROP_FPS)\n",
        "if fps2 <= 0:\n",
        "    fps2 = FPS\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "writer2 = cv2.VideoWriter(task2_output_video, fourcc, fps2, (w2, h2))\n",
        "\n",
        "frame_idx = 1\n",
        "frame_counts = []   # list of (frame_number, count)\n",
        "\n",
        "while True:\n",
        "    ok, frame = cap2.read()\n",
        "    if not ok:\n",
        "        break\n",
        "\n",
        "    # YOLO detection\n",
        "    det = yolo_model(frame, conf=conf_threshold, verbose=False)[0]\n",
        "\n",
        "    det_list = []\n",
        "    if det.boxes is not None:\n",
        "        for b in det.boxes:\n",
        "            cls_id = int(b.cls[0])\n",
        "            conf   = float(b.conf[0])\n",
        "            if cls_id != 0:     # keep only \"person\"\n",
        "                continue\n",
        "\n",
        "            x1, y1, x2, y2 = b.xyxy[0].tolist()\n",
        "            det_list.append(([x1, y1, x2 - x1, y2 - y1], conf, \"person\"))\n",
        "\n",
        "    # DeepSORT tracking\n",
        "    tracks = tracker.update_tracks(det_list, frame=frame)\n",
        "\n",
        "    # draw & count confirmed tracks\n",
        "    people_in_frame = 0\n",
        "    for trk in tracks:\n",
        "        if not trk.is_confirmed():\n",
        "            continue\n",
        "\n",
        "        draw_and_log_box(frame, frame_idx, trk, txt_file=None)  # we won't log to txt here\n",
        "        people_in_frame += 1\n",
        "\n",
        "    # store count for this frame\n",
        "    frame_counts.append((frame_idx, people_in_frame))\n",
        "\n",
        "    writer2.write(frame)\n",
        "\n",
        "    if frame_idx % 50 == 0:\n",
        "        print(f\"Task2 – processed frame {frame_idx}, count = {people_in_frame}\")\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "cap2.release()\n",
        "writer2.release()\n",
        "\n",
        "print(\"Finished Task2 tracking.\")\n",
        "print(\"Annotated Task2 video saved at:\", task2_output_video)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFVkDqB6RiOc",
        "outputId": "0bd0b28b-b118-4aa5-dd14-4c8221747055"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task2 annotated video: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task2.mp4\n",
            "Task2 counts csv: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/group50_object_tracking.csv\n",
            "Task2 – processed frame 50, count = 35\n",
            "Task2 – processed frame 100, count = 34\n",
            "Task2 – processed frame 150, count = 33\n",
            "Task2 – processed frame 200, count = 28\n",
            "Task2 – processed frame 250, count = 32\n",
            "Task2 – processed frame 300, count = 34\n",
            "Task2 – processed frame 350, count = 38\n",
            "Task2 – processed frame 400, count = 35\n",
            "Task2 – processed frame 450, count = 33\n",
            "Task2 – processed frame 500, count = 33\n",
            "Task2 – processed frame 550, count = 38\n",
            "Task2 – processed frame 600, count = 34\n",
            "Task2 – processed frame 650, count = 33\n",
            "Task2 – processed frame 700, count = 28\n",
            "Task2 – processed frame 750, count = 25\n",
            "Task2 – processed frame 800, count = 24\n",
            "Task2 – processed frame 850, count = 28\n",
            "Task2 – processed frame 900, count = 31\n",
            "Task2 – processed frame 950, count = 28\n",
            "Task2 – processed frame 1000, count = 28\n",
            "Task2 – processed frame 1050, count = 28\n",
            "Finished Task2 tracking.\n",
            "Annotated Task2 video saved at: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task2.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(task2_counts_csv, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Number\", \"Count\"])\n",
        "    for frame_num, cnt in frame_counts:\n",
        "        writer.writerow([frame_num, cnt])\n",
        "\n",
        "print(\"Saved Task2 pedestrian counts to:\", task2_counts_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M47xYgTmRqua",
        "outputId": "33fcb060-7f25-439d-9ab2-d80dcc349af4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Task2 pedestrian counts to: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/group50_object_tracking.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # trial (scale added)\n",
        "\n",
        "# # output paths for Task2\n",
        "# task2_output_video = os.path.join(object_tracking_root, \"task2.mp4\")\n",
        "# task2_counts_csv   = os.path.join(object_tracking_root, \"group50_object_tracking.csv\")\n",
        "\n",
        "# print(\"Task2 annotated video:\", task2_output_video)\n",
        "# print(\"Task2 counts csv     :\", task2_counts_csv)\n",
        "\n",
        "# conf_threshold = 0.05          # slightly lower for crowded scenes\n",
        "\n",
        "# # models\n",
        "# yolo_model = YOLO(\"yolov8l.pt\")\n",
        "# tracker = DeepSort(max_age=30, n_init=3, max_iou_distance=0.7)\n",
        "\n",
        "# cap2 = cv2.VideoCapture(task2_input_video_path)\n",
        "# if not cap2.isOpened():\n",
        "#     raise RuntimeError(\"Cannot open Task2 input video\")\n",
        "\n",
        "# w2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# h2 = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# fps2 = cap2.get(cv2.CAP_PROP_FPS)\n",
        "# if fps2 <= 0:\n",
        "#     fps2 = FPS\n",
        "\n",
        "# fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "# writer2 = cv2.VideoWriter(task2_output_video, fourcc, fps2, (w2, h2))\n",
        "\n",
        "# frame_idx = 1\n",
        "# frame_counts = []   # list of (frame_number, count)\n",
        "\n",
        "# scale = 2  # upscale factor for YOLO\n",
        "\n",
        "# while True:\n",
        "#     ok, frame = cap2.read()\n",
        "#     if not ok:\n",
        "#         break\n",
        "\n",
        "#     # ----------------------------------------------------\n",
        "#     # 1) YOLO detection on an upscaled frame\n",
        "#     # ----------------------------------------------------\n",
        "#     big_frame = cv2.resize(frame, None, fx=scale, fy=scale)\n",
        "\n",
        "#     det = yolo_model(\n",
        "#         big_frame,\n",
        "#         conf=conf_threshold,\n",
        "#         imgsz=1280,\n",
        "#         iou=0.45,\n",
        "#         verbose=False\n",
        "#     )[0]\n",
        "\n",
        "#     det_list = []\n",
        "#     person_det_count = 0\n",
        "\n",
        "#     if det.boxes is not None:\n",
        "#         for b in det.boxes:\n",
        "#             cls_id = int(b.cls[0])\n",
        "#             conf   = float(b.conf[0])\n",
        "#             if cls_id != 0:      # keep only \"person\"\n",
        "#                 continue\n",
        "\n",
        "#             x1, y1, x2, y2 = b.xyxy[0].tolist()\n",
        "\n",
        "#             # map back to original frame coordinates\n",
        "#             x1 /= scale\n",
        "#             y1 /= scale\n",
        "#             x2 /= scale\n",
        "#             y2 /= scale\n",
        "\n",
        "#             det_list.append(([x1, y1, x2 - x1, y2 - y1], conf, \"person\"))\n",
        "#             person_det_count += 1\n",
        "\n",
        "#     # *** COUNT FOR KAGGLE: number of YOLO person detections ***\n",
        "#     people_in_frame = person_det_count\n",
        "#     frame_counts.append((frame_idx, people_in_frame))\n",
        "\n",
        "#     # ----------------------------------------------------\n",
        "#     # 2) DeepSORT tracking (for visualization only)\n",
        "#     # ----------------------------------------------------\n",
        "#     tracks = tracker.update_tracks(det_list, frame=frame)\n",
        "\n",
        "#     for trk in tracks:\n",
        "#         # draw only tracks that were updated this frame\n",
        "#         if getattr(trk, \"time_since_update\", 0) != 0:\n",
        "#             continue\n",
        "#         draw_and_log_box(frame, frame_idx, trk, txt_file=None)\n",
        "\n",
        "#     writer2.write(frame)\n",
        "\n",
        "#     if frame_idx % 50 == 0:\n",
        "#         print(f\"Task2 – processed frame {frame_idx}, YOLO count = {people_in_frame}\")\n",
        "\n",
        "#     frame_idx += 1\n",
        "\n",
        "# cap2.release()\n",
        "# writer2.release()\n",
        "\n",
        "# print(\"Finished Task2 tracking.\")\n",
        "# print(\"Annotated Task2 video saved at:\", task2_output_video)\n",
        "\n",
        "# # write CSV\n",
        "# with open(task2_counts_csv, \"w\", newline=\"\") as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerow([\"Number\", \"Count\"])\n",
        "#     for frame_num, cnt in frame_counts:\n",
        "#         writer.writerow([frame_num, cnt])\n",
        "\n",
        "# print(\"Saved Task2 pedestrian counts to:\", task2_counts_csv)\n",
        "# print(\"Total frames:\", len(frame_counts))\n"
      ],
      "metadata": {
        "id": "cf6GHgaG1dG4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trial 2 (scale and division just for left corner)\n",
        "\n",
        "# Task2 images and input video\n",
        "task2_images_dir       = os.path.join(object_tracking_root, \"Task2\", \"images\")\n",
        "task2_input_video_path = os.path.join(object_tracking_root, \"task2_input.mp4\")\n",
        "\n",
        "print(\"Task2 images dir :\", task2_images_dir)\n",
        "print(\"Task2 input video:\", task2_input_video_path)\n",
        "\n",
        "# Build the Task2 input video from images (reuse your Part 1 helper)\n",
        "images_to_video(task2_images_dir, task2_input_video_path, FPS)\n",
        "\n",
        "# Output paths for Task2\n",
        "task2_output_video = os.path.join(object_tracking_root, \"task2.mp4\")\n",
        "task2_counts_csv   = os.path.join(object_tracking_root, \"group50_object_tracking.csv\")\n",
        "\n",
        "print(\"Task2 annotated video:\", task2_output_video)\n",
        "print(\"Task2 counts csv     :\", task2_counts_csv)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# YOLO + DeepSORT setup\n",
        "# ----------------------------------------------------------------------\n",
        "conf_threshold = 0.3        # low for high recall; you can tweak to 0.07 or 0.1\n",
        "tile_scale     = 2.0         # how much to zoom each tile\n",
        "\n",
        "yolo_model = YOLO(\"yolov8l.pt\")\n",
        "tracker    = DeepSort(max_age=30, n_init=3, max_iou_distance=0.7)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# NMS helpers (to merge detections from 4 tiles)\n",
        "# ----------------------------------------------------------------------\n",
        "def iou_xyxy(a, b):\n",
        "    \"\"\"\n",
        "    IoU between two boxes in (x1, y1, x2, y2) format.\n",
        "    \"\"\"\n",
        "    xa1, ya1, xa2, ya2 = a\n",
        "    xb1, yb1, xb2, yb2 = b\n",
        "\n",
        "    ix1 = max(xa1, xb1)\n",
        "    iy1 = max(ya1, yb1)\n",
        "    ix2 = min(xa2, xb2)\n",
        "    iy2 = min(ya2, yb2)\n",
        "\n",
        "    iw = max(0.0, ix2 - ix1)\n",
        "    ih = max(0.0, iy2 - iy1)\n",
        "    inter = iw * ih\n",
        "\n",
        "    if inter <= 0:\n",
        "        return 0.0\n",
        "\n",
        "    area_a = max(0.0, xa2 - xa1) * max(0.0, ya2 - ya1)\n",
        "    area_b = max(0.0, xb2 - xb1) * max(0.0, yb2 - yb1)\n",
        "    return inter / (area_a + area_b - inter + 1e-6)\n",
        "\n",
        "def nms_boxes(boxes, iou_thr=0.6):\n",
        "    \"\"\"\n",
        "    Simple NMS over a list of boxes.\n",
        "\n",
        "    boxes: list of (x1, y1, x2, y2, conf)\n",
        "    returns: filtered list of same format\n",
        "    \"\"\"\n",
        "    if not boxes:\n",
        "        return []\n",
        "\n",
        "    # sort by confidence (high to low)\n",
        "    boxes_sorted = sorted(boxes, key=lambda b: b[4], reverse=True)\n",
        "    kept = []\n",
        "\n",
        "    for box in boxes_sorted:\n",
        "        x1, y1, x2, y2, conf = box\n",
        "        keep = True\n",
        "        for kb in kept:\n",
        "            if iou_xyxy((x1, y1, x2, y2), kb[:4]) > iou_thr:\n",
        "                keep = False\n",
        "                break\n",
        "        if keep:\n",
        "            kept.append(box)\n",
        "\n",
        "    return kept\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Video I/O\n",
        "# ----------------------------------------------------------------------\n",
        "cap2 = cv2.VideoCapture(task2_input_video_path)\n",
        "if not cap2.isOpened():\n",
        "    raise RuntimeError(\"Cannot open Task2 input video\")\n",
        "\n",
        "w2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h2 = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fps2 = cap2.get(cv2.CAP_PROP_FPS)\n",
        "if fps2 <= 0:\n",
        "    fps2 = FPS\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "writer2 = cv2.VideoWriter(task2_output_video, fourcc, fps2, (w2, h2))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Main loop: tile → detect → merge → track → count\n",
        "# ----------------------------------------------------------------------\n",
        "frame_idx     = 1\n",
        "frame_counts  = []   # list of (frame_number, count)\n",
        "\n",
        "while True:\n",
        "    ok, frame = cap2.read()\n",
        "    if not ok:\n",
        "        break\n",
        "\n",
        "    H, W = frame.shape[:2]\n",
        "    mid_x = W // 2\n",
        "    mid_y = H // 2\n",
        "\n",
        "    # 4 tiles: TL, TR, BL, BR\n",
        "    tiles = [\n",
        "        (0,      0,      mid_x, mid_y),  # top-left\n",
        "        (mid_x,  0,      W,     mid_y),  # top-right\n",
        "        (0,      mid_y,  mid_x, H),      # bottom-left\n",
        "        (mid_x,  mid_y,  W,     H),      # bottom-right\n",
        "    ]\n",
        "\n",
        "    all_boxes = []   # will store (x1, y1, x2, y2, conf) in full-frame coords\n",
        "\n",
        "    # 1) Run YOLO on each upscaled tile\n",
        "    for (tx1, ty1, tx2, ty2) in tiles:\n",
        "        tile = frame[ty1:ty2, tx1:tx2]\n",
        "        if tile.size == 0:\n",
        "            continue\n",
        "\n",
        "        # upscale tile\n",
        "        tile_big = cv2.resize(tile, None, fx=tile_scale, fy=tile_scale)\n",
        "\n",
        "        # YOLO on tile\n",
        "        det_tile = yolo_model(\n",
        "            tile_big,\n",
        "            conf=conf_threshold,\n",
        "            imgsz=1280,\n",
        "            iou=0.5,\n",
        "            verbose=False\n",
        "        )[0]\n",
        "\n",
        "        if det_tile.boxes is None:\n",
        "            continue\n",
        "\n",
        "        for b in det_tile.boxes:\n",
        "            cls_id = int(b.cls[0])\n",
        "            conf   = float(b.conf[0])\n",
        "            if cls_id != 0:       # keep only 'person'\n",
        "                continue\n",
        "\n",
        "            bx1, by1, bx2, by2 = b.xyxy[0].tolist()\n",
        "\n",
        "            # scale back to original frame coordinates and add tile offset\n",
        "            x1 = bx1 / tile_scale + tx1\n",
        "            y1 = by1 / tile_scale + ty1\n",
        "            x2 = bx2 / tile_scale + tx1\n",
        "            y2 = by2 / tile_scale + ty1\n",
        "\n",
        "            all_boxes.append((x1, y1, x2, y2, conf))\n",
        "\n",
        "    # 2) NMS across tiles to avoid double-counting near boundaries\n",
        "    final_boxes = nms_boxes(all_boxes, iou_thr=0.6)\n",
        "\n",
        "    # 3) Build DeepSORT detections from merged boxes & count people\n",
        "    det_list = []\n",
        "    for (x1, y1, x2, y2, conf) in final_boxes:\n",
        "        det_list.append(([x1, y1, x2 - x1, y2 - y1], conf, \"person\"))\n",
        "\n",
        "    # Kaggle count = number of merged detections\n",
        "    people_in_frame = len(final_boxes)\n",
        "    frame_counts.append((frame_idx, people_in_frame))\n",
        "\n",
        "    # 4) DeepSORT tracking for visualization only (IDs on video)\n",
        "    tracks = tracker.update_tracks(det_list, frame=frame)\n",
        "\n",
        "    for trk in tracks:\n",
        "        if not trk.is_confirmed():\n",
        "            continue\n",
        "        # reuse your helper; no logging here, only drawing\n",
        "        draw_and_log_box(frame, frame_idx, trk, txt_file=None)\n",
        "\n",
        "    writer2.write(frame)\n",
        "\n",
        "    if frame_idx % 50 == 0:\n",
        "        print(f\"Task2 – processed frame {frame_idx}, count = {people_in_frame}\")\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "cap2.release()\n",
        "writer2.release()\n",
        "\n",
        "print(\"Finished Task2 tracking.\")\n",
        "print(\"Annotated Task2 video saved at:\", task2_output_video)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Write Kaggle CSV: Number,Count\n",
        "# ----------------------------------------------------------------------\n",
        "with open(task2_counts_csv, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Number\", \"Count\"])\n",
        "    for frame_num, cnt in frame_counts:\n",
        "        writer.writerow([frame_num, cnt])\n",
        "\n",
        "print(\"Saved Task2 pedestrian counts to:\", task2_counts_csv)\n",
        "print(\"Total frames:\", len(frame_counts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljMw6oZs9Qz2",
        "outputId": "d51ff00f-e2f0-4cc3-8595-e63ce7459c57"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task2 images dir : /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/Task2/images\n",
            "Task2 input video: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task2_input.mp4\n",
            "Found 1050 images in /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/Task2/images\n",
            "Target frame size: (1920, 1080)\n",
            "Video saved to: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task2_input.mp4\n",
            "Task2 annotated video: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task2.mp4\n",
            "Task2 counts csv     : /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/group50_object_tracking.csv\n",
            "Task2 – processed frame 50, count = 38\n",
            "Task2 – processed frame 100, count = 37\n",
            "Task2 – processed frame 150, count = 36\n",
            "Task2 – processed frame 200, count = 35\n",
            "Task2 – processed frame 250, count = 42\n",
            "Task2 – processed frame 300, count = 48\n",
            "Task2 – processed frame 350, count = 51\n",
            "Task2 – processed frame 400, count = 47\n",
            "Task2 – processed frame 450, count = 43\n",
            "Task2 – processed frame 500, count = 40\n",
            "Task2 – processed frame 550, count = 43\n",
            "Task2 – processed frame 600, count = 33\n",
            "Task2 – processed frame 650, count = 42\n",
            "Task2 – processed frame 700, count = 34\n",
            "Task2 – processed frame 750, count = 33\n",
            "Task2 – processed frame 800, count = 34\n",
            "Task2 – processed frame 850, count = 37\n",
            "Task2 – processed frame 900, count = 42\n",
            "Task2 – processed frame 950, count = 42\n",
            "Task2 – processed frame 1000, count = 44\n",
            "Task2 – processed frame 1050, count = 43\n",
            "Finished Task2 tracking.\n",
            "Annotated Task2 video saved at: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/task2.mp4\n",
            "Saved Task2 pedestrian counts to: /content/drive/MyDrive/McGill/2025/Fall 2025/ECSE 415/A5/Object_Tracking/group50_object_tracking.csv\n",
            "Total frames: 1050\n"
          ]
        }
      ]
    }
  ]
}